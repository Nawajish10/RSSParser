{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70781e45-33bc-472b-a6b7-6af7aaf61e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a521fa-3835-4131-bc5e-dde3831ee432",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_feeds=[\n",
    "    \"http://rss.cnn.com/rss/cnn_topstories.rss\",\n",
    "    \"http://qz.com/feed\",\n",
    "    \"http://feeds.foxnews.com/foxnews/politics\",\n",
    "    \"http://feeds.reuters.com/reuters/businessNews\",\n",
    "    \"http://feeds.feedburner.com/NewshourWorld\",\n",
    "    \"https://feeds.bbci.co.uk/news/world/asia/india/rss.xml\"]\n",
    "def fetch_and_parse_feed(feed_url):\n",
    "    return feedparser.parse(feed_url)\n",
    "\n",
    "def extract_article_data(entry):\n",
    "    title = entry.title\n",
    "    content = entry.summary\n",
    "    published_date = entry.published\n",
    "    link = entry.link\n",
    "\n",
    "def process_feeds():\n",
    "    articles = []\n",
    "    for feed in rss_feeds:\n",
    "        feed_data = fetch_and_parse_feed(feed)\n",
    "        for entry in feed_data.entries:\n",
    "            article = extract_article_data(entry)\n",
    "            articles.append(article)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812ae63",
   "metadata": {},
   "source": [
    "CONNECTING THE DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af89cb7f-dbc7-4347-abed-545396314095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection_cext.CMySQLConnection object at 0x00000230EF9BF200>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from multiprocessing import connection\n",
    "import mysql.connector # type: ignore\n",
    "\n",
    "mydb= mysql.connector.Connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"Nawajish@2001\",\n",
    "    database=\"project\"\n",
    ")\n",
    "print(mydb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ec566",
   "metadata": {},
   "source": [
    "CREATING THE CURSOR TO INTERACT WITH THE DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "209a1cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('information_schema',)\n",
      "('mysql',)\n",
      "('performance_schema',)\n",
      "('project',)\n",
      "('sys',)\n"
     ]
    }
   ],
   "source": [
    "mycursor=mydb.cursor()\n",
    "mycursor.execute(\"SHOW DATABASES\")\n",
    "for db in mycursor:\n",
    "    print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "159a1ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akhta\\AppData\\Local\\Temp\\ipykernel_13044\\1975263946.py:2: DeprecationWarning: The Tix Tk extension is unmaintained, and the tkinter.tix wrapper module is deprecated in favor of tkinter.ttk\n",
      "  from tkinter.tix import TEXT\n",
      "C:\\Users\\akhta\\AppData\\Local\\Temp\\ipykernel_13044\\1975263946.py:7: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base= declarative_base()\n"
     ]
    }
   ],
   "source": [
    "from sqlite3 import Timestamp\n",
    "from tkinter.tix import TEXT\n",
    "from sqlalchemy import create_engine, ForeignKey, Column,String,Integer,CHAR # type: ignore\n",
    "from sqlalchemy.ext.declarative import declarative_base # type: ignore\n",
    "from sqlalchemy.orm import sessionmaker # type: ignore\n",
    "\n",
    "Base= declarative_base()\n",
    "\n",
    "#Definign the article models \n",
    "class news_article(Base):\n",
    "    __tablename__ = 'news_articles'\n",
    "\n",
    "    id= Column(\"id\" ,Integer, primary_key =True)\n",
    "    unique_id=Column(\"unique_id\", Integer) \n",
    "    title = Column(\"title\", String, nullable=False) \n",
    "    content = Column(\"content\", String, nullable=False)  \n",
    "    link = Column(\"link\", String, nullable=False)\n",
    "    published_date = Column(\"Published_date\", String)\n",
    "    source_url =  Column(\"source_url\", String)\n",
    "\n",
    "#create the engine\n",
    "def create_db_engine():\n",
    "    engine= create_engine('mysql+mysqlclient://root:Nawajish@2001@localhost/project')\n",
    "    Base.metadata.create_all(engine)\n",
    "    return engine\n",
    "\n",
    "# Creating a session\n",
    "def create_session(engine):\n",
    "    session= sessionmaker(bind= engine)\n",
    "    return session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51bfaab",
   "metadata": {},
   "source": [
    "INSERTING THE PARSED DATA INTO THE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "075b1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_articles(session,news_article):\n",
    "    for data in news_article:\n",
    "        new_data = News_article(\n",
    "            title= data['title'],\n",
    "            content= data['content'],\n",
    "            link= data['link'],\n",
    "            published_date = data['published_date'],\n",
    "            source_url=data['source_url']\n",
    "        )\n",
    "        session.add(news_article)\n",
    "\n",
    "    session.commit()\n",
    "    print(f\"Inserted {len(news_article)} articles into the database.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c3dcc",
   "metadata": {},
   "source": [
    "USING CELERY FOR ASYNCHRONOUS PROCESSING OF FETCHED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5701fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from celery import Celery\n",
    "\n",
    "app= Celery('news_processor',broker='redis://localhost:6379/0')\n",
    "\n",
    "@app.task\n",
    "def process_new_articles(article):\n",
    "    category=classify_article(article['title'], article['content']) # type: ignore\n",
    "    update_article_with_category=(article['unique_id', category])\n",
    "\n",
    "    print(f\"Processed article: {article['title']} assigned to category {category}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd8151",
   "metadata": {},
   "source": [
    "CELERY WORKER TO HANDLE ARTICLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b4a756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import session\n",
    "\n",
    "\n",
    "def update_article_with_category(unique_id, category):\n",
    "    article = session.query(NewsArticle).filter_by(unique_id=unique_id).first()\n",
    "    if article:\n",
    "        article.category = category\n",
    "        session.commit()\n",
    "\n",
    "def classify_article(title, content):\n",
    "    # Example rule-based classification logic\n",
    "    if \"protest\" in title.lower() or \"riot\" in content.lower():\n",
    "        return \"Terrorism / Protest / Political Unrest / Riot\"\n",
    "    elif \"earthquake\" in content.lower() or \"flood\" in title.lower():\n",
    "        return \"Natural Disasters\"\n",
    "    elif \"positive\" in content.lower() or \"inspiring\" in title.lower():\n",
    "        return \"Positive / Uplifting\"\n",
    "    else:\n",
    "        return \"Others\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a8870",
   "metadata": {},
   "source": [
    "TRIGGER PARSING AND QUEUE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f96c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feeds():\n",
    "    for feed in rss_feeds:\n",
    "        feed_data=fetch_and_parse_feed(feed)\n",
    "        for entry in feed_data.entries:\n",
    "            article=extract_article_data(entry)\n",
    "            process_new_articles.delay(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737e116",
   "metadata": {},
   "source": [
    "Logging and ERROR HANDLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0c9b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='news_processing.log', level=logging.INFO,\n",
    "                    format='%(asctime)s %(message)s')\n",
    "\n",
    "def log_event(event_message):\n",
    "    logging.info(event_message)\n",
    "\n",
    "def log_error(error_message):\n",
    "    logging.error(error_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18531b1d",
   "metadata": {},
   "source": [
    "ERROR HANDLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "666e2ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_parse_feed(feed_url):\n",
    "    try:\n",
    "        return feedparser.parse(feed_url)\n",
    "    except Exception as e:\n",
    "        log_error(f\"Failed to fetch feed: {feed_url} due to {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87995fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
